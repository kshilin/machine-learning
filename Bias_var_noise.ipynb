{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d578806c-2b41-4995-b96b-b0bd815e3764",
   "metadata": {},
   "source": [
    "Давайте попробуем разобраться в природе ошибок возникающих в наших моделях.\n",
    "\n",
    "Пусть наша целевая зависимость имеет вид:\n",
    "$$ y(x)= f(x)+\\varepsilon(x), \\;\\; \\varepsilon(x)\\sim \\mathcal{N}(0,\\sigma^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09aa8c0-152f-4f2e-9744-5574f596e86e",
   "metadata": {},
   "source": [
    "То есть истинная зависимость  $y(x)$ предсказывается  $a(x) = f (x)$ в точках обучающей \n",
    "выборки  с точностью до шума $\\varepsilon(x)$, в каждой точке шум свой, шумы в разных \n",
    "точках независимы и одинаково нормально распределены с мат. ожиданием 0 и дисперсией $\\sigma^2$. По идее если шум не нормален, то у нас есть некоторые проблемы с моделью ...\n",
    "\n",
    "Дальше мы будем брать математическое ожидание квадрата отклонениянашего алгоритма $a(x)$ от истинного значения  $y(x)$ в фиксированной точке $x$:\n",
    "\n",
    "$$\\mathbf{E}_{\\{x_i,f(x_i)+\\varepsilon(x_)\\}_{i=1}^n} \\left(y(x)-a(x)\\right)^2 $$\n",
    "\n",
    "по всем данным и всем настройкам алгоритма (некоторые алгоритмы имеют стохастическую\n",
    "природу, например случайный лес).\n",
    "Для упрощения формул не будем указывать конкретную точку: $a \\equiv a(x)$ и $y \\equiv y(x)$\n",
    "\n",
    "Тогда, обозначенное выше матожидание равно:\n",
    "\n",
    "$$\\mathbf{E}(y-a)^2 = \\mathbf{E}(y^2+a^2-2ya)=$$\n",
    "$$= \\mathbf{E}y^2-(\\mathbf{E}y)^2+(\\mathbf{E}y)^2+\\mathbf{E}a^2-(\\mathbf{E}a)^2+(\\mathbf{E}a)^2 - 2\\mathbf{E}ya =$$\n",
    "$$= \\left(\\mathbf{E}y^2-(\\mathbf{E}y)^2\\right) + \\left(\\mathbf{E}a^2-(\\mathbf{E}a)^2\\right)+ \n",
    "    \\left((\\mathbf{E}y)^2 - 2\\mathbf{E}ya +(\\mathbf{E}a)^2\\right) =$$\n",
    "$$ = \\mathbf{D}y + \\mathbf{D}a + (\\mathbf{E}y)^2 - 2\\mathbf{E}ya +(\\mathbf{E}a)^2 = ... $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2c8860-0d83-4d29-ac93-2b56ee0fd807",
   "metadata": {},
   "source": [
    "Теперь посмотрим на важный переход от $2\\mathbf{E}ya \\to 2\\hat y \\mathbf{E}a$:\n",
    "\n",
    "$$ \\mathbf{E}ya = \\mathbf{E}\\left( (f(x)+\\varepsilon(x)) a(x) \\right) = \\mathbf{E}\\left( (f(x)a(x) \\right) +\n",
    " \\mathbf{E}\\left( (\\varepsilon(x)a(x) \\right)$$\n",
    "\n",
    "Но:\n",
    "$$ \\mathbf{E}\\left( (\\varepsilon(x)a(x) \\right) = \\mathbf{E} (\\varepsilon(x)) \\mathbf{E} (a(x)) = 0\\cdot \\mathbf{E} (a(x)) = 0$$\n",
    "Так как $f(x)$ по сути является константой (значение истинной зависимости в точке $x$):\n",
    "$$\\mathbf{E}\\left( (f(x)a(x) \\right) = f(x)\\mathbf{E} (a(x))$$\n",
    "Тогда:\n",
    "$$ \\mathbf{E}ya = f \\mathbf{E}a$$\n",
    "Продолжим:\n",
    "$$ ... = \\mathbf{D}y + \\mathbf{D}a + (\\mathbf{E}y)^2 - 2f\\mathbf{E}a +(\\mathbf{E}a)^2 =$$\n",
    "$$ = \\mathbf{D}y + \\mathbf{D}a + (f-\\mathbf{E}a)^2 = $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaac657-56d9-437f-80c9-6addce0feec1",
   "metadata": {},
   "source": [
    "Давайте определимся с названиями резульиаиов в нашем результате:\n",
    "\n",
    "***Ошибка модели = Шум в данных + Разброс ответов модели + Отклонение от истины (смещение) в квадрате***\n",
    "\n",
    "- шум в данных - $\\mathbf{D}y = \\sigma^2$\n",
    "- разброс ответов модели $\\mathbf{D}a = \\text{variance(a)}$\n",
    "- oтклонение от истины (смещение) в квадрате $(f-\\mathbf{E}a)^2 = \\text{bias}^2(f,a)$\n",
    "\n",
    "$$\\mathbf{E}(y-a)^2 = \\sigma^2 + \\text{variance(a)} +\\text{bias}^2(f,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d17435-b11b-4691-a69b-a0cb92eda3c3",
   "metadata": {},
   "source": [
    "Слагаемое «шум» $\\sigma^2$ связано с шумом в самих данных (и тут мы ничего не сможем сделать), а вот два остальных –\n",
    "связаны с используемыми  алгоритмами.\n",
    "\n",
    "Достаточно очевидно, что разброс характеризует разнообразие алгоритмов (из-за случайности обучающей выборки, в том числе шума, и стохастической природы настройки), а смещение – способность модели алгоритмов настраиваться на целевую зависимость."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a5677-dbd6-4e0f-900e-10403cbaf7bd",
   "metadata": {},
   "source": [
    "<div class=\"row\">\n",
    "    <div class=\"column\">\n",
    "    <img src=\"pict/Bias_big_l.png\" width=\"400\">\n",
    "    <img src=\"pict/Bias_big.png\" width=\"400\" >\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<div class=\"row\">\n",
    "    <div class=\"column\">\n",
    "    <img src=\"pict/Bias_small_l.png\" width=\"400\">\n",
    "    <img src=\"pict/Bias_small.png\" width=\"400\" >\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "Если провести аналогию, что алгоритмы – это \"снайпер\" стреляющей по мишени.\n",
    "Лучший стрелок будет иметь небольшое смещение и разброс – его высрелы\n",
    "ложатся кучно «в яблочко», \n",
    "если у игрока большое смешение, то они\n",
    "сгруппированы около другой точки (вдали от центра мишени) - видимо у него сбит прицел ;)\n",
    "Если у стрелка большой разброс, то они ложатся совсем не кучно. \n",
    "\n",
    "<img src=\"pict/b_var.png\" width=\"400\">\n",
    "\n",
    "Здесь нужно понимать, что в отличии от стрельбы с алгоритмами машинного\n",
    "обучения все несколько сложнее. Смещение нельзя просто «подвинуть».\n",
    "Поэтому для устранения смещения приходится переходить к другой (например, более\n",
    "сложной) модели, а у неё может быть уже другой разброс.\n",
    "\n",
    "Часто для демонстрации данных эффектов используют следующий рисунок:\n",
    "\n",
    "<img src=\"pict/over.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aece8f-6c49-45fc-bd2f-25d9ae15efc5",
   "metadata": {},
   "source": [
    "Илюстрация показывает эффект, который в литературе называется bias-variance trade-off: чем выше сложность обучаемой модели, тем меньше её смещение и тем больше разброс, и поэтому общая ошибка на тестовой выборке имеет вид U-образной кривой. \n",
    "\n",
    "С падением смещения модель всё лучше запоминает обучающую выборку, поэтому слишком сложная модель будет иметь нулевую ошибку на тренировочных данных и большую ошибку на тесте. \n",
    "\n",
    "Этот график призван показать, что существует оптимальная сложность модели, при которой соблюдается баланс между переобучением и недообучением и ошибка при этом минимальна.\n",
    "\n",
    "Существует достаточное количество подтверждений bias-variance trade-off для непараметрических моделей.\n",
    "\n",
    "Однако, как показывают последние исследования, непременное возрастание разброса при убывании смещения не является абсолютно истинным предположением. Например, для нейронных сетей с ростом их сложности может происходить снижение и разброса, и смещения [Белкин и др. (Belkin et al., 2019)](https://arxiv.org/pdf/1812.11118). \n",
    "\n",
    "<img src=\"pict/over_big.png\" width=\"700\">\n",
    "\n",
    "Слева — классический bias-variance trade-off: убывающая часть кривой соответствует недообученной модели, а возрастающая — переобученной. А на правой картинке — график, называемый в статье double descent risk curve. На нём изображена эмпирически наблюдаемая авторами зависимость тестовой ошибки нейросетей от мощности множества входящих в них параметров (H). \n",
    "\n",
    "Этот график разделён на две части пунктирной линией, которую авторы называют interpolation threshold. Эта линия соответствует точке, в которой в нейросети стало достаточно параметров, чтобы без особых усилий почти идеально запомнить всю обучающую выборку. \n",
    "\n",
    "Часть до достижения interpolation threshold соответствует «классическому» режиму обучения моделей: когда у модели недостаточно параметров, чтобы сохранить обобщающую способность при почти полном запоминании обучающей выборки. А часть после достижения interpolation threshold соответствует «современным» возможностям обучения моделей с огромным числом параметров. \n",
    "\n",
    "На этой части графика ошибка монотонно убывает с ростом количества параметров у нейросети. Авторы также наблюдают похожее поведение и для «древесных» моделей: Random Forest и бустинга над решающими деревьями. Для них эффект проявляется при одновременном росте глубины и числа входящих в ансамбль деревьев."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c72368-c3a6-4067-92fd-f5b74ccd4269",
   "metadata": {},
   "source": [
    "Некоторое общее правило какие модели какие ошибки:\n",
    "\n",
    "Практически все линейные модели - низкое смещение, дисперсия уменьшается регуляризацией с увеличением смещения. Увеличение количества признаков ученьшает смещение за счет увеличения дисперсии.\n",
    "\n",
    "В моделях k-ближайших соседей большое значение k ведёт к большому смещению и низкой дисперсии.\n",
    "\n",
    "В деревьях решений глубина дерева определяет дисперсию. Деревья решений обычно обрезаются для контроля дисперсии (речь об еденичном дереве!).\n",
    "\n",
    "Один из способов разрешения дилеммы смещение-дисперсия — использование смешанных моделей и ансамблевого обучения. \n",
    "Например, бустинг комбинирует несколько «слабых» (с высоким смещением) моделей в сборку, которая имеет более низкое смещение, чем каждая из индивидуальных моделей, в то время как бэггинг комбинирует «строгое»  обучение так, что уменьшается дисперсия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb85cba-d236-44ad-955a-70b8ccaa60dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ad93d0-1b3d-4772-a18e-bb18487aada5",
   "metadata": {},
   "source": [
    "# Продолжим про ансамбли\n",
    "\n",
    "Основная идея: у нас есть **слабые алгоритмы** (переученные или недоученные) и мы хотим их как-то собрать воедино и тем самым усилить наше решение:\n",
    "$$b_1(x), b_2(x), ... , b_N(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce0dbc7-473d-4ed2-95a5-a55da6f39e12",
   "metadata": {},
   "source": [
    "1. Бэггинг (для деревьев) - усредняем переученные глубокие деревья решения, при этом деревья должны быть различны (за счет бутстрапа), желательно максимально независимо:\n",
    "$$a_N(x) = \\dfrac{1}{N}\\sum_{i=1}^N b_i(x)$$\n",
    "2. Бустинг (сейчас мы тут)\n",
    "3. Войтинг\n",
    "4. Стекинг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f001021e-4fd2-43e2-8437-0a1d0c9d8788",
   "metadata": {},
   "source": [
    "# Идея бустинга\n",
    "\n",
    "Необходимо решить задачу $\\{x_i \\to y_i\\}_{i=1}^m$\n",
    "\n",
    "1. Возьмем простые базовые модели\n",
    "2. Будем строить композицию **последовательно**\n",
    "3. Каждая следующая модель будет строиться так, чтобы максимально корректировоть ошибку ранее сделанного шага.\n",
    "\n",
    "***Аналогия: бэггинг это обучения в студентов в группе, бустинг - обучение команды под решение задачи, где каждый в чем-то силен, а в чем-то нет.***\n",
    "\n",
    "   Основная причина применения  бустинга заключается в последовательном достижении оптимума для алгорится за меньшее число шагов, чем при бэггинге, одно тут плохо с параллелизацией. Следовательно (забегаая вперед), количество делевьев для бустинга является гиперпараметром."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea2c7b-efc6-4017-b7b2-1655cc4af4b2",
   "metadata": {},
   "source": [
    " Итак, мы будем искать неосредненный (как ранее в беггинге алгоритм):\n",
    "$$a_N(x) = \\sum_{i=1}^N b_i(x)$$\n",
    "\n",
    "Сделаем первый шаг:\n",
    "$$\\sum_{i=1}^m L(y_i, b_1(x_i)) \\to \\underset{b_1(x)}{\\min}$$\n",
    "\n",
    "Тогда для $N$-ой модели:\n",
    "$$\\sum_{i=1}^m L(y_i, a_{N-1}(x_i)+b_N(x_i)) \\to \\underset{b_N(x)}{\\min},$$\n",
    "то есть для $a_{N-1}(x_i)$ уже обученной модели мы добавляем $b_N(x)$, что бы ее прогноз уменьшил уже существующею ошибку.\n",
    "\n",
    "Как же строить модель $b_N$, что бы она улучшила итоговый результат построения модели $a_N$  по сравнению $a_{N-1}$? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f2cc22-9aa1-4a2e-a145-8f9c362351cd",
   "metadata": {},
   "source": [
    "## Самый простой вариант бустинга для понимания\n",
    "\n",
    "Рассмотрим самый простой вариант нашего бустинга на квадратичной ошибке:\n",
    "$$\\dfrac{1}{N}\\sum_{i=1}^N (a_{N-1}(x_i)+b_N(x_i)-y_i)^2 \\to \\underset{b_N(x)}{\\min},$$\n",
    "\n",
    "Тогда первый шаг:\n",
    "$$\\dfrac{1}{N}\\sum_{i=1}^N (b_1(x_i)-y_i)^2 \\to \\underset{b_1(x)}{\\min},$$\n",
    "\n",
    "Тогда второй шаг:\n",
    "$$\\dfrac{1}{N}\\sum_{i=1}^N \\left(b_1(x_i) + b_2(x_i)-y_i)\\right)^2 \\to \\underset{b_2(x)}{\\min} \\Rightarrow$$\n",
    "$$\\dfrac{1}{N}\\sum_{i=1}^N \\left(b_2(x_i)-(y_i-b_1(x_i))\\right)^2 \\to \\underset{b_2(x)}{\\min},$$\n",
    "то есть вторая модель $b_2(x_i)$ обучается на новой целевой переменной, не на $y_i$, а на разнице между $y_i-b_1(x_i)$.\n",
    "Если вторая модель $b_2(x_i)$ идельно компенсирует разницу, то получим нулевую ошибку.\n",
    "\n",
    "Третий шаг: \n",
    "$$\\dfrac{1}{N}\\sum_{i=1}^N \\left(b_2(x_i)-(y_i-b_1(x_i)-b_2(x_i))\\right)^2 \\to \\underset{b_3(x)}{\\min},$$\n",
    "\n",
    "В итоге мы за маленькое количество шагов, можем найти прекрасное решение,  \n",
    "но есть **маленькая проблемка** - переобучение ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c19ffe-2369-4da1-8d7d-e4fb34554196",
   "metadata": {},
   "source": [
    "## Сложности с функцией потерь\n",
    "\n",
    "Давайте посмотрим на **\"любую\"** функцию потерь:\n",
    "$$\\sum_{i=1}^m L(y_i, a_{N-1}(x_i)+b_N(x_i)) \\to \\underset{b_N(x)}{\\min}$$\n",
    "\n",
    "Расмотрим логистическую регрессию, в качестве примера:\n",
    "$$a_N(x)=\\text{sign} \\sum_{i=1}^N b_n(x)$$\n",
    "$$\\sum_{i=1}^m L(y,z) = \\log(1+\\exp(-y z))$$\n",
    "\n",
    "\n",
    "Попробуем найти остатки как в MSE:\n",
    "$$\\sum_{i=1}^m L(y,z) = \\log(1+\\exp(-(y_i-a_N(x_i)b_N(x_i)) )) \\to \\underset{b_N(x)}{\\min}$$\n",
    "где $y$ это $y_i-a_N(x_i)$, а $z$ - $b_N(x_i)$ тогда получим, что:\n",
    "- если построенная ранее модель хорошо отработала, то он не участвует в обучении $y_i = a_N(x_i)$ и получили 0 в произведении.\n",
    "- если построенная ранее модель плохо отработала, то $y_i-a_N(x_i)=\\pm 2$. Ок. Ну ладно , давайте теперь расмотрим **только** для ответов с ошибками:\n",
    "\n",
    "$$\\sum_{i=1}^m L(y,z) = \\log\\left(1+\\exp\\left(-\\left(\\dfrac{y_i-a_N(x_i)}{2}b_N(x_i)\\right) \\right)\\right) \\to \\underset{b_N(x)}{\\min}$$\n",
    "Если $y_i \\neq a_N(x_i)$, то модель учится искать корректный класс, однако ...\n",
    "\n",
    "- $y_i=+1,\\;\\;\\; \\sum_{i=1}^N b_n(x_i)= -0.5$(это класс -1), нам нужно $b_n(x_i)>0.5$ - корректура маленькая\n",
    "- $y_i=+1, \\;\\;\\; \\sum_{i=1}^N b_n(x_i)= -100$(это класс -1), нам нужно $b_n(x_i)>100$ - корректура большая,\n",
    "а наша модель не знает насколько нужно скорректироваться и тут все стало плохо ...\n",
    "\n",
    "Поэтому нам нужен другой подход."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f6de28-0e0d-45ae-b0ef-f6d74e59d3b8",
   "metadata": {},
   "source": [
    "## Градиентный спуск\n",
    "\n",
    "Ошибка на $x_i$ при прогнозе новой модели, равном $z_i$:\n",
    "$$\\sum_{i=1}^m L(y_i, a_{N-1}(x_i)+z_i)$$\n",
    "\n",
    "Посчитаем производную:\n",
    "$s_i^{(N)} = -\\dfrac{\\partial}{\\partial z}L(y_i,z_i)\\Big|_{z = a_{N-1}(x_i)}$, тогда\n",
    "- знак показывает сторону сдвига прогноза $x_i$\n",
    "- величина показывает как сильно двигаться для уменьшения ошибки\n",
    "- если ошибка меньше порога, то менять что-либо смысла нет.\n",
    "\n",
    "  Тогда мы построили прекрасную модель машинного обучения будущего:\n",
    "\n",
    "$$\\dfrac{1}{N}\\sum_{i=1}^N \\left(b_N(x_i)-s_i^{(N)}\\right)^2 \\to \\underset{b_N(x)}{\\min}$$\n",
    "где $s_i^{(N)} = -\\dfrac{\\partial}{\\partial z}L(y_i,z)\\Big|_{z = a_{N-1}(x_i)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e069d82-22fd-475e-a64f-befc9eca5dbd",
   "metadata": {},
   "source": [
    "### Градиентный бустинг MSE\n",
    "\n",
    "$$s_i^{(N)} = -\\dfrac{\\partial}{\\partial z}L(y_i,z_i)\\Big|_{z = a_{N-1}(x_i)} = -\\dfrac{1\\partial}{2\\partial z}(z-y^2)\\Big|_{z = a_{N-1}(x_i)}=$$\n",
    "$$= -(a_N(x_i)-y_i)=y_i-a_N(x_i)$$\n",
    "\n",
    "что это напоминает ...\n",
    "\n",
    "$$\\sum_{i=1}^m L(b_N(x_i) - (y_i - a_{N-1}(x_i)))^2 \\to \\underset{b_N(x)}{\\min},$$\n",
    "\n",
    "И это оцень просто и бысто для MSE:  \n",
    "$y_i = 10, \\;\\; a_{N-1}(x_i)=5$ тогда $s_i = 5$    \n",
    "$y_i = 10, \\;\\; a_{N-1}(x_i)=-5$ тогда $s_i = -15$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46356673-1d9f-4321-9369-5a968a6b0f3d",
   "metadata": {},
   "source": [
    "### Градиентный бустинг логистической функции потерь\n",
    "\n",
    "$$s_i^{(N)} = -\\dfrac{\\partial}{\\partial z}L(y_i,z_i)\\Big|_{z = a_{N-1}(x_i)} = -\\dfrac{\\partial}{\\partial z}\n",
    "\\log(1+\\exp(-y_i z))\\Big|_{z = a_{N-1}(x_i)}=$$\n",
    "$$=\\dfrac{y_i}{1+\\exp(y_i a_{N-1}(x_i))}$$\n",
    "\n",
    "$$\\sum_{i=1}^m L\\left(b_N(x_i) - \\dfrac{y_i}{1+\\exp(y_i a_{N-1}(x_i))}\\right)^2 \\to \\underset{b_N(x)}{\\min}$$\n",
    "\n",
    "$y_i$ и $a_{N-1}(x_i)$ значительно больше 0:   $\\dfrac{y_i}{1+\\exp(y_i a_{N-1}(x_i))} \\to 0$    \n",
    "разница $y_i$ и $a_{N-1}(x_i)$ большая отрицательная:   $\\dfrac{y_i}{1+\\exp(y_i a_{N-1}(x_i))} \\to \\pm 1$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763eb7a-016a-4940-b204-a05958dfe63a",
   "metadata": {},
   "source": [
    "# Гиперпараметры бустнинга\n",
    "\n",
    "В ходе экспериментов с моделями установлено:\n",
    "1. Градиентный бустинг уменьшает смешение моделей\n",
    "2. Разброс может увеличиваться\n",
    "\n",
    "Следствие: \n",
    "- нельзя брать глубокие модели, обычно глубиной 2-5 `max_depth`.\n",
    "- нужно подбирать число деревьев `n_estimators`\n",
    "\n",
    "Важные, но влияют чуть меньше:\n",
    "\n",
    "3. простые модели (базовые) очень простые, они могут или медленно и слишком быстро шагать, поэтому нужно регулировать шаг `learning_rate`: $a_N = a_{N-1}(x_i) + \\eta\\; b_N(x_i))$, $\\eta \\in (0, +\\infty)$\n",
    "4. рандомизация по признакам `subsample` в интервале $(0.0, 1.0]$ (обычно замедляюд шаг и увеличивают рандомизацию), при рандомизации можно счить OOB-ошибки. Обычно увеличивает смешение и уменьшает дисперсию.\n",
    "\n",
    "Важно помнить, что длинна шага коррелирует с количеством деревьев!\n",
    "\n",
    "Важно запомнить - НИКАКИХ РЕШЕТОК!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae08041-23f2-452e-91c8-a007df5c76f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148d48d-d88e-443b-86bf-706ef1f1cf47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3104a96-63e0-409d-95fb-159a6527e696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

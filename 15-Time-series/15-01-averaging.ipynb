{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Несколько важных слов об усреднении\n",
    "\n",
    "Ошибка на новых данных состоит из:\n",
    "1. Шума\n",
    "2. Смещения (Bias)\n",
    "3. Разброса (Variance)\n",
    "\n",
    "**Шум** показывает ошибку \"лучшей в Мире\" моделисреди остальных. Шум принципиально не устраним, он существует независимо и непредсказуемо.\n",
    "\n",
    "**Смещение** это отклонение средних ответов нашей модели от ответов лучшей модели. Смещение показывает насколько мы хорошо апроксимируем модель.\n",
    "\n",
    "**Разброс** это дисперсия нашей модели.\n",
    "\n",
    "<img src=\"pict/b-v.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ремарка**<br>\n",
    "Существует ряд \"обычных\" закономерностей, которые нужно помнить при исаользовании моделей:\n",
    "1. У линейных моделей обычно большое смещение и имеют маленький разброс (плохо востанавливают закономерности); \n",
    "2. У решающего дерева обычно низкое смещение и большой разброс (хорошо востанавливаются закономерности); \n",
    "3. Прм усреднении деревьев - смещение не меняется, а вот разброс падает 1/N, если N - независимых алгоритмов.\n",
    "\n",
    "Итак обсуди следующие важные моменты.\n",
    "\n",
    "**Первое – переобучение (overfitting)** – явление, когда ошибка на тестовой выборке заметно больше ошибки на обучающей. <br>\n",
    "Это главная проблема машинного обучения: если бы такого эффекта не было (ошибка на тесте примерно совпадала с ошибкой на обучении), то всё обучение сводилось бы к минимизации ошибки на обучении.\n",
    "\n",
    "**Второе – недообучение (underfitting)** – явление, когда ошибка на обучающей выборке достаточно большая, часто говорят «не удаётся настроиться на выборку». Такой странный термин объясняется тем, что недообучение при настройке алгоритмов итерационными методами (например, в случайном лесу) можно наблюдать, когда сделано слишком маленькое число итераций (слишком мало деревьев), т.е. «не успели обучиться».\n",
    "\n",
    "**Третье – сложность (complexity)** модели алгоритмов  – оценивает, насколько разнообразно семейство алгоритмов в модели с точки зрения их функциональных свойств (например, способности настраиваться на выборки). Повышение сложности (т.е. использование более сложных моделей) решает проблему недообучения и вызывает переобучение.\n",
    "\n",
    "<img src=\"pict/error.png\" width=\"900\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного математики. Пусть наша целевая зависимость имеет вид:\n",
    "$$y(x)= f(x)+\\varepsilon, \\;\\;\\; \\varepsilon \\sim Norm(0,\\sigma^2),\\; y \\sim Norm(f(x),\\sigma^2)$$\n",
    "\n",
    "Построим алгоритм $a=a(x)$ (у нас это многочлен n-степени), посмотрим чему равно математическое ожидание квадрата отклонения ответа алгоритма от истинного значения:\n",
    "\n",
    "$$E(y-a)=E(y^2-a^2-2ya)= $$\n",
    "$$ =E(y^2)+E(a^2)-2E(ya)+E(y)^2-E(y)^2+E(a)^2-E(a)^2 =$$\n",
    "так как $D(y) = E\\left(\\left(y-E(y)\\right)^2\\right)= E\\left((y-f(x))^2\\right) = E\\left((f(x)+\\varepsilon-f(x))^2\\right) = \\sigma^2$\n",
    "$$ = D(y)+ D(a)+E(y)^2+E(a)^2-2E(ya)=$$\n",
    "так как $E(y) = E(f(x)+\\varepsilon = E(f(x))+E(\\varepsilon) = E(f(x))= f(x)$ \n",
    "$$= D(y)+ D(a)+f(x)^2 + E(a)^2 - 2f(x)E(a) =$$\n",
    "$$ = \\left(E(f(x)-a)\\right)^2 + D(y)+ D(a)= $$\n",
    "$$ = Bias(f(x),a)^2 + \\sigma^2 +variance(a)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак **разбросом** мы назвали дисперсию ответов алгоритмов, а **смещением (bias)** – мат. ожидание разности между истинным ответом и выданным алгоритмом.\n",
    "\n",
    "Мы получили, что ошибка раскладывается на три составляющие. Первая связана с шумом в самих данных, а вот две остальные связаны с используемой моделью алгоритмов. Понятно, что разброс характеризует разнообразие алгоритмов (из-за случайности обучающей выборки, в том числе шума, и стохастической природы настройки), а смещение – способность модели алгоритмов настраиваться на целевую зависимость. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проиллюстрируем это. На риспоказаны различные полиномы первой степени, они настроены на разных обучающих выборках. В точке $x=0.5$ ответы алгоритмов являются случайными величинами, они немного «разбросаны» (есть variance), а также они сильно смещены (есть bias) относительно правильного ответа (который, кстати, даже если нам и известен, то с точностью до шума).\n",
    "\n",
    "<img src=\"pict/bias_1.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На рисунке изображены уже полиномы второй степени (настроенные на тех же выборках). В точке $x=0.5$ у них сильно меньше смещение и чуть меньше разброс. Видно, что они совсем неплохо описывают целевую зависимость во всех точках.\n",
    "<img src=\"pict/bias_2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим самую частую иллюстрацию, которую приводят при объяснении разброса и смещения При увеличении сложности модели (например, степени многочлена) ошибка на независимом контроле сначала падает, потом начинает увеличиваться. \n",
    "\n",
    "Обычно это связывают с уменьшением смещения (в сложных моделях очень много алгоритмов, поэтому наверняка найдутся те, которые хорошо описывают целевую зависимость) и увеличением разброса (в сложных моделях больше алгоритмов, а следовательно, и больше разброс).\n",
    "\n",
    "<img src=\"pict/overfit.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простых моделей характерно недообучение (они слишком простые, не могут описать целевую зависимость и имеют большое смещение), для сложных – переобучение (алгоритмов в модели слишком много, при настройке мы выбираем ту, которая хорошо описывает обучающую выборку, но из-за сильного разброса она может допускать большую ошибку на тесте)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

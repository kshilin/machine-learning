{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Внимание!!! Важно, что бы файлы с данными и исполняемый файл находились в одной папке, \n",
    "# тогда пути к тестовым и тренировочным наборам будут содержать только имена файлов.\n",
    "# \n",
    "# В пути к тренировочным и тестовым данным запрежается использовать абсалютную адресацию, \n",
    "# то есть адресацию, в которой присутствуют имена папок. Путь должен содержать только имя файла.\n",
    "#\n",
    "# Напоминание: под моделью машинного обучения понимаются все действия с исходными данными, \n",
    "# которые необходимо произвести, что бы сопоставить признаки целевому значению."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Область работы 1 (библиотеки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данный блок в области 1 НЕ выполняется преподавателем\n",
    "# \n",
    "# данный блок предназначен только для подключения необходимых библиотек\n",
    "# запрещается подключать библиотеки в других блоках\n",
    "# запрещается скрывать предупреждения системы\n",
    "# установка дополнительных библиотек размещается прямо здесь (обязательно закоментированы)\n",
    "# pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Normalizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Область работы 2 (поиск  модели .... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данный блок(и) НЕ выполняются преподавателем в области 2\n",
    "# блок(и) предназначены для поиска лучшей модели \n",
    "# должен быть понятен и очевиден отбор параметров модели\n",
    "# оставляйте свои комментарии и разъяснения\n",
    "# \n",
    "# Запрещается размещать данные блоки за пределами обасти 2\n",
    "# Все блоки данной области должны быть выполнены\n",
    "#\n",
    "# ЗАПРЕЩАЕТСЯ ИСПОЛЬЗОВАТЬ ТЕСТОВЫЙ НАБОР\n",
    "#\n",
    "# Путь к тренировочному набору\n",
    "# \n",
    "path_train = 'train.csv' # содержит только имя файла, без имен папок !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Для начала я подгружаю данные из csv файла и дропаю дубликаты(их в наборе не оказалось), целевая переменная для меня хорошее вино, вину с качеством >= 7 присваиваю единицу, плохому - ноль. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.091</td>\n",
       "      <td>42.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.99380</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.36</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.037</td>\n",
       "      <td>34.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0.99200</td>\n",
       "      <td>3.13</td>\n",
       "      <td>0.57</td>\n",
       "      <td>12.3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.053</td>\n",
       "      <td>54.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>0.99610</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.43</td>\n",
       "      <td>9.3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.048</td>\n",
       "      <td>17.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.98953</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.37</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.034</td>\n",
       "      <td>5.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.99256</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.54</td>\n",
       "      <td>10.4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.22</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0.030</td>\n",
       "      <td>10.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99300</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.87</td>\n",
       "      <td>11.6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.048</td>\n",
       "      <td>19.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.99270</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.43</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.029</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.99118</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.50</td>\n",
       "      <td>12.5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.29</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.030</td>\n",
       "      <td>32.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.98913</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.33</td>\n",
       "      <td>13.7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.021</td>\n",
       "      <td>31.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.98919</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0.49</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1725 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               6.4              0.17         0.34             1.5      0.091   \n",
       "1               8.0              0.42         0.36             5.0      0.037   \n",
       "2               6.7              0.27         0.25             8.0      0.053   \n",
       "3               5.2              0.21         0.31             1.7      0.048   \n",
       "4               7.6              0.48         0.37             1.2      0.034   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1720            6.9              0.39         0.22             4.3      0.030   \n",
       "1721            6.4              0.34         0.10             1.1      0.048   \n",
       "1722            7.0              0.53         0.43             6.1      0.029   \n",
       "1723            5.9              0.17         0.29             3.1      0.030   \n",
       "1724            5.5              0.34         0.26             2.2      0.021   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    42.0                 135.0  0.99380  3.25       0.49   \n",
       "1                    34.0                 101.0  0.99200  3.13       0.57   \n",
       "2                    54.0                 202.0  0.99610  3.22       0.43   \n",
       "3                    17.0                  61.0  0.98953  3.24       0.37   \n",
       "4                     5.0                  57.0  0.99256  3.05       0.54   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1720                 10.0                 102.0  0.99300  3.00       0.87   \n",
       "1721                 19.0                  84.0  0.99270  3.21       0.38   \n",
       "1722                  6.0                  76.0  0.99118  3.08       0.50   \n",
       "1723                 32.0                 123.0  0.98913  3.41       0.33   \n",
       "1724                 31.0                 119.0  0.98919  3.55       0.49   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.6        7  \n",
       "1        12.3        7  \n",
       "2         9.3        5  \n",
       "3        12.0        7  \n",
       "4        10.4        3  \n",
       "...       ...      ...  \n",
       "1720     11.6        4  \n",
       "1721      9.8        5  \n",
       "1722     12.5        8  \n",
       "1723     13.7        7  \n",
       "1724     13.0        8  \n",
       "\n",
       "[1725 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(path_train)\n",
    "data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предварительно я использовал pandas_profiling для проверки данных, density коррелирует сильно с другими столбцами, но удаление/какая-то работа с датафреймом не приводит к улучшению результата, поэтому заниматься датасатанизмом я не стал, прикладывать репорт тоже не вижу смысла, из-за него ломается отрисовка графиков seaborn и matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классы дизбалансные, хорошего вина 35%... с его определением возникают проблемы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol, quality]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['quality'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сплитую данные на тренировочные и тестовые, изменял параметры тест сайза 30% выдаёт лучшие результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['quality'] = data['quality'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['quality']), data.quality, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Все модели я прогонял сначала отдельно, они давали +- одинаковые результаты: Logreg даёт более высокий пресижн и более низкий рекол, NB выдаёт высокий рекол, но относительно малый пресижн, KNN - что-то среднее между ними, поэтому я решил прогнать все алгоритмы со всеми возможными параметрами в одной решётке... \n",
    "## Кстати о метрике, нужно с ней определиться: Accuracy сразу мимо, остаются 3 основных кандидата: Recall, precision и f1, f1 усредняет рекол и пресижн, думаю, в условиях соревнования нужно рисковать. Что хуже: выбирать хорошее вино под видом плохого или упускать хорошее вино? Ошибка первого рода или второго? Как мне кажется намного хуже отдать много денег за \"хорошее\" вино, а получить совсем не то, что ожидал, наверное, не этого люди хотят от покупки дорогого и хорошего вина, максимизирую $Precision$ !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = [MinMaxScaler(), StandardScaler(), RobustScaler(), Normalizer()] # все скалеры, о которых говорили на паре\n",
    "parameters = [\n",
    "    {\n",
    "        'preprocessing': scalers,\n",
    "        'classifier': [KNeighborsClassifier()], # соседи\n",
    "        'classifier__n_neighbors': np.arange(1, 25), # 25 соседей хватает, на тестовых прогонах было 2 лучших результата ~ 10 и ~ 20\n",
    "        'classifier__weights': ['uniform'], # убрал distance, модель переобучается на тренировочном наборе\n",
    "        'classifier__p': [1, 2 , 3] # метрики: манхетенская, евклидова, миньковского\n",
    "    },\n",
    "    {\n",
    "        'preprocessing': scalers,\n",
    "        'classifier': [LogisticRegression()], # логистическая регрессия\n",
    "        'classifier__penalty': ['l1'], # штраф l1 удаляет признаки практически не влияющие на ответ (количество признаков уменьшается)\n",
    "        'classifier__solver': ['liblinear', 'saga'] \n",
    "    },\n",
    "    {\n",
    "        'preprocessing': scalers,\n",
    "        'classifier': [LogisticRegression()], # регрессия\n",
    "        'classifier__penalty': ['l2'], # штраф l2 штрафует признаки практически не влияющие на ответ (количество признаков не меняется)\n",
    "        'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'] \n",
    "    },\n",
    "    {\n",
    "        'preprocessing': scalers,\n",
    "        'classifier': [LogisticRegression()], # регрессия\n",
    "        'classifier__penalty': ['none'], # не штрафуем\n",
    "        'classifier__solver': ['newton-cg', 'lbfgs', 'sag', 'saga'] \n",
    "    },\n",
    "    {\n",
    "        'preprocessing': scalers,\n",
    "        'classifier': [GaussianNB()] #  гауссовский наивный байес\n",
    "    },\n",
    "    {\n",
    "        'preprocessing': scalers,\n",
    "        'classifier': [MultinomialNB()], # еще какой-то наивный байес\n",
    "        'classifier__alpha': np.linspace(0.1, 1, 10)\n",
    "    },\n",
    "    {\n",
    "        'preprocessing': scalers,\n",
    "        'classifier': [BernoulliNB()], # наивный байес бернулли\n",
    "        'classifier__alpha': np.linspace(0.1, 1, 10)\n",
    "    }\n",
    "]\n",
    "## сила регуляризации C не стал менять, результаты не улучшались, либо я неправильно подбирал баланс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создаю пайплайн, кросс валидацию решил выбрать $StratifiedKFold$, изначально ставил шаффл сплит, но после дополнительных прогонов, StratifiedKFold показал результаты лучше\n",
    "# Много предупреждений из-за скалеров, не всем алгоритмам нравятся отрицательные значения, но что поделаешь, придется посмотреть на много красных строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "100 fits failed out of a total of 2080.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\for conda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\for conda\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"D:\\for conda\\lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\for conda\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"D:\\for conda\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 772, in fit\n",
      "    self._count(X, Y)\n",
      "  File \"D:\\for conda\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 894, in _count\n",
      "    check_non_negative(X, \"MultinomialNB (input X)\")\n",
      "  File \"D:\\for conda\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1489, in check_non_negative\n",
      "    raise ValueError(\"Negative values in data passed to %s\" % whom)\n",
      "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [0.70171462 0.7000583  0.70419396 0.66111244 0.70089846 0.70915949\n",
      " 0.70502383 0.65694935 0.70668016 0.70253421 0.70834676 0.66689757\n",
      " 0.71747197 0.71331573 0.70919036 0.67602963 0.71165255 0.71000994\n",
      " 0.71332259 0.6768355  0.7116697  0.70915949 0.72575357 0.6768355\n",
      " 0.72329824 0.71998903 0.7108844  0.6826652  0.71582593 0.71580193\n",
      " 0.71912486 0.6859607  0.71583279 0.71911114 0.7216042  0.68677343\n",
      " 0.72659374 0.71996159 0.71499606 0.68181475 0.72742018 0.71333288\n",
      " 0.72660403 0.68679401 0.73073283 0.71912143 0.72740647 0.6859607\n",
      " 0.7166901  0.70423854 0.71168684 0.68841604 0.72495113 0.71750283\n",
      " 0.71167998 0.6992147  0.72907994 0.72412469 0.72245465 0.69672165\n",
      " 0.72412812 0.71418333 0.71251329 0.68678029 0.73158671 0.71831213\n",
      " 0.71333288 0.69672508 0.73903158 0.72162134 0.71335688 0.68677343\n",
      " 0.73158671 0.71833613 0.72080519 0.70004801 0.73244059 0.71668324\n",
      " 0.71501663 0.69260313 0.73077741 0.72744762 0.7075649  0.69426631\n",
      " 0.72162477 0.72083262 0.71664209 0.70337094 0.72661431 0.71999246\n",
      " 0.7183087  0.68595384 0.73241315 0.72082233 0.71998217 0.6851274\n",
      " 0.72245808 0.71418333 0.71416275 0.70170433 0.72910394 0.71915915\n",
      " 0.72246494 0.69343987 0.73488563 0.72661774 0.70921779 0.69760639\n",
      " 0.72493399 0.71996845 0.72823291 0.69756524 0.72410411 0.7174994\n",
      " 0.71583965 0.69922156 0.72907994 0.7174994  0.70837077 0.70004458\n",
      " 0.7282432  0.7249237  0.71748568 0.69423202 0.73322245 0.71418676\n",
      " 0.71253043 0.69342958 0.72825692 0.72578787 0.70589486 0.69507561\n",
      " 0.72076403 0.72409725 0.72495113 0.69423202 0.72494085 0.71500291\n",
      " 0.7075649  0.6917664  0.73073283 0.72246151 0.71004767 0.68265492\n",
      " 0.71498234 0.719142   0.73157299 0.70087788 0.72907651 0.72497171\n",
      " 0.71583622 0.69841569 0.73158328 0.72413155 0.71419019 0.69342958\n",
      " 0.72907308 0.73405576 0.72493742 0.68680086 0.72741676 0.73076026\n",
      " 0.71334316 0.69757553 0.7307294  0.72660746 0.71750283 0.69841226\n",
      " 0.72825006 0.72659717 0.72576386 0.68846747 0.72244093 0.72661431\n",
      " 0.71583622 0.70091904 0.72657659 0.72660746 0.71997874 0.70007544\n",
      " 0.7332156  0.72161449 0.73488906 0.68930078 0.7348582  0.73654539\n",
      " 0.71501663 0.6909571  0.73319502 0.72329138 0.72661431 0.69676966\n",
      " 0.72741333 0.72743047 0.7315627  0.69425946 0.72905593 0.7431741\n",
      " 0.71832585 0.69676966 0.73650424 0.72743047 0.72826721 0.70091561\n",
      " 0.72576043 0.73322588 0.73488563 0.69674565 0.73155242 0.72990638\n",
      " 0.72412469 0.69757553 0.7464456  0.72328109 0.71748911 0.70172491\n",
      " 0.73736154 0.73240287 0.72742704 0.6975721  0.73154899 0.73239601\n",
      " 0.72909708 0.70256164 0.73816399 0.73488906 0.72577758 0.70504784\n",
      " 0.73487192 0.72824663 0.73488221 0.68929049 0.73651109 0.73237886\n",
      " 0.72495456 0.6967628  0.73236857 0.73901787 0.72080861 0.69758239\n",
      " 0.73819828 0.73405919 0.73073969 0.69590892 0.73071568 0.73238915\n",
      " 0.73240973 0.69510991 0.72656973 0.74150406 0.72577072 0.6934433\n",
      " 0.73900758 0.72742018 0.72659031 0.69177326 0.73568122 0.72989266\n",
      " 0.73241315 0.69096053 0.73071911 0.73156613 0.71915915 0.70089503\n",
      " 0.73983402 0.72410068 0.72991667 0.6984054  0.73403518 0.73486506\n",
      " 0.73489249 0.70007887 0.73154899 0.73570179 0.72660746 0.70090189\n",
      " 0.73734783 0.73237543 0.73488906 0.69177326 0.73899386 0.73651795\n",
      " 0.73488563 0.69840883 0.73735469 0.73984431 0.71832585 0.70005487\n",
      " 0.73984431 0.74896266 0.74647989 0.65451459 0.74233051 0.74730633\n",
      " 0.74400055 0.65534104 0.73901787 0.74896608 0.74813621 0.64623641\n",
      " 0.73901787 0.74813621 0.74813621 0.64623641 0.74150406 0.74813621\n",
      " 0.74813621 0.64789273 0.73901787 0.74813621 0.74896266 0.64623641\n",
      " 0.73901787 0.74813621 0.74813621 0.64623641 0.74730633 0.74730633\n",
      " 0.74730633 0.73736497 0.74813621 0.74730633 0.74730633 0.72740647\n",
      " 0.74730633 0.74730633 0.74979596 0.70751346 0.74813621 0.74813621\n",
      " 0.74896608 0.68764446 0.70667673 0.70667673 0.70667673 0.58413978\n",
      " 0.64954563        nan        nan 0.64954563 0.64954563        nan\n",
      "        nan 0.64954563 0.64954563        nan        nan 0.64954563\n",
      " 0.64954563        nan        nan 0.64954563 0.64954563        nan\n",
      "        nan 0.64954563 0.64954563        nan        nan 0.64954563\n",
      " 0.64954563        nan        nan 0.64954563 0.64954563        nan\n",
      "        nan 0.64954563 0.64954563        nan        nan 0.64954563\n",
      " 0.64954563        nan        nan 0.64954563 0.65203525 0.69922499\n",
      " 0.71495491 0.64954563 0.65203525 0.69922499 0.71495491 0.64954563\n",
      " 0.65203525 0.69922499 0.71495491 0.64954563 0.65203525 0.69922499\n",
      " 0.71495491 0.64954563 0.6503755  0.69922499 0.71495491 0.64954563\n",
      " 0.6503755  0.69922499 0.71329515 0.64954563 0.6503755  0.69922499\n",
      " 0.71329515 0.64954563 0.6503755  0.69922499 0.71329515 0.64954563\n",
      " 0.6503755  0.69922499 0.71329515 0.64954563 0.6503755  0.69922499\n",
      " 0.71329515 0.64954563]\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the train scores are non-finite: [1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.86143318 0.86619509 0.86847467 0.83885302 0.86329568 0.87054506\n",
      " 0.86619702 0.83905899 0.86039777 0.86868299 0.86081228 0.8411311\n",
      " 0.85231981 0.85376973 0.85811884 0.83160278 0.85356204 0.85998498\n",
      " 0.84859074 0.82932342 0.84714039 0.86267714 0.85356247 0.82849698\n",
      " 0.81027387 0.81545415 0.82104571 0.79224858 0.81462406 0.8227031\n",
      " 0.82104421 0.79017647 0.81358822 0.82394619 0.8206282  0.79431918\n",
      " 0.81172572 0.82187473 0.81918236 0.78997007 0.81586715 0.82249391\n",
      " 0.81441466 0.78520774 0.8169017  0.81897317 0.81358543 0.78707152\n",
      " 0.79598151 0.79909139 0.7990901  0.77671269 0.79805598 0.80219612\n",
      " 0.80198629 0.77029382 0.79971165 0.80654738 0.80012036 0.76863451\n",
      " 0.79494545 0.80136796 0.7988792  0.76987996 0.79722717 0.79929564\n",
      " 0.79473927 0.76470205 0.80053916 0.80322981 0.79722288 0.76407985\n",
      " 0.79163132 0.79370472 0.79121853 0.75268647 0.79287506 0.78956157\n",
      " 0.79163239 0.75517609 0.78935239 0.78852423 0.78624787 0.75724648\n",
      " 0.78873363 0.78686963 0.78748925 0.76491059 0.79018247 0.78893981\n",
      " 0.78707603 0.75496862 0.78873298 0.79018269 0.78169129 0.75724498\n",
      " 0.78023901 0.78044862 0.78148296 0.75165385 0.78210558 0.78272756\n",
      " 0.77423294 0.7499956  0.78293288 0.78355507 0.77443955 0.75123741\n",
      " 0.77692766 0.78003411 0.77589182 0.75165449 0.78624744 0.78417469\n",
      " 0.77278366 0.75331209 0.78065673 0.78645405 0.77153992 0.75310377\n",
      " 0.77298984 0.77713449 0.77236872 0.7495826  0.78624572 0.7783763\n",
      " 0.7680194  0.74730516 0.77506453 0.77237065 0.76677866 0.74668297\n",
      " 0.77340542 0.7736116  0.769677   0.75331102 0.78127463 0.78024029\n",
      " 0.76843433 0.75124127 0.77713556 0.7748562  0.76367157 0.74792521\n",
      " 0.77174718 0.77154035 0.77071284 0.74792692 0.77775368 0.77299006\n",
      " 0.76491252 0.74481897 0.7756835  0.77257684 0.76470655 0.7439906\n",
      " 0.7713331  0.7686418  0.76636437 0.74709855 0.7781669  0.77444148\n",
      " 0.76511806 0.74523413 0.77547796 0.77381907 0.76615626 0.7417121\n",
      " 0.76781085 0.76553578 0.76346453 0.7435761  0.77029854 0.77195529\n",
      " 0.76242504 0.74109076 0.77174825 0.76698377 0.7624259  0.73549813\n",
      " 0.7690533  0.76512127 0.76180972 0.74191914 0.77216297 0.77216211\n",
      " 0.76449737 0.74088222 0.77257705 0.76946889 0.7624274  0.73549856\n",
      " 0.76097641 0.76139263 0.75745717 0.73570581 0.77009065 0.76594686\n",
      " 0.76035508 0.7346674  0.76822815 0.76905566 0.76346174 0.73218378\n",
      " 0.76574003 0.76387861 0.75745867 0.73777599 0.76822772 0.76988339\n",
      " 0.75870069 0.73404907 0.76988361 0.76843412 0.75994186 0.7346704\n",
      " 0.76139156 0.76201375 0.75787297 0.73611946 0.76491359 0.76636351\n",
      " 0.75497184 0.73197739 0.76718931 0.76387732 0.7591137  0.73446336\n",
      " 0.7591137  0.76408543 0.75911606 0.73736277 0.76636437 0.76677652\n",
      " 0.7628417  0.73488001 0.76594729 0.76532788 0.76222401 0.73487701\n",
      " 0.7599425  0.76367135 0.75725099 0.73342731 0.76429226 0.76677416\n",
      " 0.76242805 0.73467212 0.76532681 0.76739871 0.7609792  0.732184\n",
      " 0.7597346  0.7611871  0.75870112 0.73384224 0.76491402 0.76760575\n",
      " 0.76698527 0.73363434 0.76615475 0.76677888 0.76118881 0.73508512\n",
      " 0.75724992 0.75973568 0.75497313 0.72866433 0.76222165 0.76698377\n",
      " 0.76346238 0.7330128  0.76574132 0.7694691  0.75973696 0.73280619\n",
      " 0.74544417 0.75041333 0.75103552 0.65430846 0.74606529 0.749792\n",
      " 0.74979264 0.65596563 0.74316609 0.75041354 0.74979221 0.64623156\n",
      " 0.74316609 0.75041354 0.74979221 0.64623156 0.74544353 0.75103445\n",
      " 0.75082805 0.64623135 0.74316609 0.75041354 0.74999946 0.64623156\n",
      " 0.74337313 0.75041354 0.74999989 0.64623156 0.75145003 0.75145003\n",
      " 0.75145003 0.7460655  0.75145003 0.75145003 0.75145003 0.73633058\n",
      " 0.75145003 0.75145003 0.75041397 0.70940195 0.75124277 0.75145003\n",
      " 0.75227733 0.69656079 0.71416428 0.71416428 0.71416428 0.59299006\n",
      " 0.64954441        nan        nan 0.64954441 0.64954441        nan\n",
      "        nan 0.64954441 0.64954441        nan        nan 0.64954441\n",
      " 0.64954441        nan        nan 0.64954441 0.64954441        nan\n",
      "        nan 0.64954441 0.64954441        nan        nan 0.64954441\n",
      " 0.64954441        nan        nan 0.64954441 0.64954441        nan\n",
      "        nan 0.64954441 0.64954441        nan        nan 0.64954441\n",
      " 0.64954441        nan        nan 0.64954441 0.65368691 0.70360356\n",
      " 0.71768566 0.64954441 0.65368691 0.70360356 0.71768566 0.64954441\n",
      " 0.65368691 0.70360356 0.71768566 0.64954441 0.65368691 0.70360356\n",
      " 0.71768566 0.64954441 0.65368691 0.70360356 0.71768566 0.64954441\n",
      " 0.65368691 0.70360356 0.71768566 0.64954441 0.65368691 0.70360356\n",
      " 0.71768566 0.64954441 0.65368691 0.70360356 0.71768566 0.64954441\n",
      " 0.65368691 0.70360356 0.71768566 0.64954441 0.65368691 0.70360356\n",
      " 0.71768566 0.64954441]\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "D:\\for conda\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('preprocessing', MinMaxScaler()), ('classifier', KNeighborsClassifier())])\n",
    "cv = StratifiedKFold(n_splits=5) \n",
    "grid = GridSearchCV(pipe, parameters, cv=cv, return_train_score=True)\n",
    "grid.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Посмотрим, какие параметры \"победили\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'classifier': LogisticRegression(),\n",
       "  'classifier__penalty': 'none',\n",
       "  'classifier__solver': 'sag',\n",
       "  'preprocessing': RobustScaler()},\n",
       " 0.7497959603580124,\n",
       " 0.777992277992278)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_, grid.best_score_, grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Однозачно нулевой класс я обрабатываю лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.89      0.83       323\n",
      "           1       0.77      0.59      0.67       195\n",
      "\n",
      "    accuracy                           0.78       518\n",
      "   macro avg       0.77      0.74      0.75       518\n",
      "weighted avg       0.78      0.78      0.77       518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid.best_params_, grid.best_score_, grid.score(X_test, y_test)\n",
    "print(classification_report(y_test, grid.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выведу датафрейм с результатами работы моей решётки, главное не отобрать переобученную модель ( у неё на трейн скорах будут 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>326</th>\n",
       "      <th>330</th>\n",
       "      <th>297</th>\n",
       "      <th>310</th>\n",
       "      <th>289</th>\n",
       "      <th>298</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>...</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>369</th>\n",
       "      <th>370</th>\n",
       "      <th>373</th>\n",
       "      <th>374</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.016599</td>\n",
       "      <td>0.007604</td>\n",
       "      <td>0.016805</td>\n",
       "      <td>0.007601</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>0.005205</td>\n",
       "      <td>0.007596</td>\n",
       "      <td>0.004201</td>\n",
       "      <td>0.005202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>0.004401</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>0.004201</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>0.004401</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>0.004401</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.004601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.00049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_classifier</th>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>...</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_classifier__n_neighbors</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_classifier__p</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_classifier__weights</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_preprocessing</th>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>...</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>RobustScaler()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_classifier__penalty</th>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>l2</td>\n",
       "      <td>l2</td>\n",
       "      <td>l1</td>\n",
       "      <td>l2</td>\n",
       "      <td>l2</td>\n",
       "      <td>l2</td>\n",
       "      <td>l2</td>\n",
       "      <td>l2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_classifier__solver</th>\n",
       "      <td>sag</td>\n",
       "      <td>saga</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>sag</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_classifier__alpha</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>{'classifier': LogisticRegression(), 'classifi...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'classifier__a...</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'classifier__a...</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'classifier__a...</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'classifier__a...</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'classifier__a...</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'classifier__a...</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'classifier__a...</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'classifier__a...</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'classifier__a...</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'classifier__a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.731405</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>0.768595</td>\n",
       "      <td>0.768595</td>\n",
       "      <td>0.768595</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.768595</td>\n",
       "      <td>0.768595</td>\n",
       "      <td>0.768595</td>\n",
       "      <td>0.768595</td>\n",
       "      <td>0.768595</td>\n",
       "      <td>0.768595</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_score</th>\n",
       "      <td>0.771784</td>\n",
       "      <td>0.767635</td>\n",
       "      <td>0.771784</td>\n",
       "      <td>0.767635</td>\n",
       "      <td>0.771784</td>\n",
       "      <td>0.767635</td>\n",
       "      <td>0.767635</td>\n",
       "      <td>0.767635</td>\n",
       "      <td>0.767635</td>\n",
       "      <td>0.767635</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_score</th>\n",
       "      <td>0.763485</td>\n",
       "      <td>0.759336</td>\n",
       "      <td>0.759336</td>\n",
       "      <td>0.759336</td>\n",
       "      <td>0.759336</td>\n",
       "      <td>0.759336</td>\n",
       "      <td>0.759336</td>\n",
       "      <td>0.759336</td>\n",
       "      <td>0.759336</td>\n",
       "      <td>0.759336</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_score</th>\n",
       "      <td>0.717842</td>\n",
       "      <td>0.721992</td>\n",
       "      <td>0.717842</td>\n",
       "      <td>0.717842</td>\n",
       "      <td>0.713693</td>\n",
       "      <td>0.717842</td>\n",
       "      <td>0.717842</td>\n",
       "      <td>0.717842</td>\n",
       "      <td>0.717842</td>\n",
       "      <td>0.717842</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>0.749796</td>\n",
       "      <td>0.748966</td>\n",
       "      <td>0.748966</td>\n",
       "      <td>0.748963</td>\n",
       "      <td>0.748963</td>\n",
       "      <td>0.748136</td>\n",
       "      <td>0.748136</td>\n",
       "      <td>0.748136</td>\n",
       "      <td>0.748136</td>\n",
       "      <td>0.748136</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.022595</td>\n",
       "      <td>0.020197</td>\n",
       "      <td>0.022149</td>\n",
       "      <td>0.022181</td>\n",
       "      <td>0.022654</td>\n",
       "      <td>0.021341</td>\n",
       "      <td>0.021341</td>\n",
       "      <td>0.021341</td>\n",
       "      <td>0.021341</td>\n",
       "      <td>0.021341</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>397</td>\n",
       "      <td>397</td>\n",
       "      <td>397</td>\n",
       "      <td>397</td>\n",
       "      <td>397</td>\n",
       "      <td>397</td>\n",
       "      <td>397</td>\n",
       "      <td>397</td>\n",
       "      <td>397</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_train_score</th>\n",
       "      <td>0.75544</td>\n",
       "      <td>0.751295</td>\n",
       "      <td>0.750259</td>\n",
       "      <td>0.749223</td>\n",
       "      <td>0.751295</td>\n",
       "      <td>0.748187</td>\n",
       "      <td>0.750259</td>\n",
       "      <td>0.748187</td>\n",
       "      <td>0.749223</td>\n",
       "      <td>0.749223</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_train_score</th>\n",
       "      <td>0.744041</td>\n",
       "      <td>0.748187</td>\n",
       "      <td>0.74715</td>\n",
       "      <td>0.748187</td>\n",
       "      <td>0.745078</td>\n",
       "      <td>0.748187</td>\n",
       "      <td>0.74715</td>\n",
       "      <td>0.748187</td>\n",
       "      <td>0.74715</td>\n",
       "      <td>0.750259</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_train_score</th>\n",
       "      <td>0.745342</td>\n",
       "      <td>0.746377</td>\n",
       "      <td>0.746377</td>\n",
       "      <td>0.743271</td>\n",
       "      <td>0.744306</td>\n",
       "      <td>0.743271</td>\n",
       "      <td>0.746377</td>\n",
       "      <td>0.743271</td>\n",
       "      <td>0.745342</td>\n",
       "      <td>0.743271</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_train_score</th>\n",
       "      <td>0.752588</td>\n",
       "      <td>0.756729</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.754658</td>\n",
       "      <td>0.755694</td>\n",
       "      <td>0.754658</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.754658</td>\n",
       "      <td>0.755694</td>\n",
       "      <td>0.754658</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_train_score</th>\n",
       "      <td>0.754658</td>\n",
       "      <td>0.758799</td>\n",
       "      <td>0.754658</td>\n",
       "      <td>0.754658</td>\n",
       "      <td>0.755694</td>\n",
       "      <td>0.754658</td>\n",
       "      <td>0.754658</td>\n",
       "      <td>0.754658</td>\n",
       "      <td>0.757764</td>\n",
       "      <td>0.756729</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_score</th>\n",
       "      <td>0.750414</td>\n",
       "      <td>0.752277</td>\n",
       "      <td>0.750414</td>\n",
       "      <td>0.749999</td>\n",
       "      <td>0.750413</td>\n",
       "      <td>0.749792</td>\n",
       "      <td>0.750414</td>\n",
       "      <td>0.749792</td>\n",
       "      <td>0.751034</td>\n",
       "      <td>0.750828</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_train_score</th>\n",
       "      <td>0.004782</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>0.004303</td>\n",
       "      <td>0.004946</td>\n",
       "      <td>0.00436</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>0.00436</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.00468</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28 rows × 416 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             326  \\\n",
       "mean_fit_time                                                             0.0158   \n",
       "std_fit_time                                                            0.000982   \n",
       "mean_score_time                                                         0.002006   \n",
       "std_score_time                                                          0.000006   \n",
       "param_classifier                                            LogisticRegression()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                               RobustScaler()   \n",
       "param_classifier__penalty                                                   none   \n",
       "param_classifier__solver                                                     sag   \n",
       "param_classifier__alpha                                                      NaN   \n",
       "params                         {'classifier': LogisticRegression(), 'classifi...   \n",
       "split0_test_score                                                       0.727273   \n",
       "split1_test_score                                                       0.768595   \n",
       "split2_test_score                                                       0.771784   \n",
       "split3_test_score                                                       0.763485   \n",
       "split4_test_score                                                       0.717842   \n",
       "mean_test_score                                                         0.749796   \n",
       "std_test_score                                                          0.022595   \n",
       "rank_test_score                                                                1   \n",
       "split0_train_score                                                       0.75544   \n",
       "split1_train_score                                                      0.744041   \n",
       "split2_train_score                                                      0.745342   \n",
       "split3_train_score                                                      0.752588   \n",
       "split4_train_score                                                      0.754658   \n",
       "mean_train_score                                                        0.750414   \n",
       "std_train_score                                                         0.004782   \n",
       "\n",
       "                                                                             330  \\\n",
       "mean_fit_time                                                           0.016599   \n",
       "std_fit_time                                                            0.000494   \n",
       "mean_score_time                                                         0.001603   \n",
       "std_score_time                                                          0.000487   \n",
       "param_classifier                                            LogisticRegression()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                               RobustScaler()   \n",
       "param_classifier__penalty                                                   none   \n",
       "param_classifier__solver                                                    saga   \n",
       "param_classifier__alpha                                                      NaN   \n",
       "params                         {'classifier': LogisticRegression(), 'classifi...   \n",
       "split0_test_score                                                       0.727273   \n",
       "split1_test_score                                                       0.768595   \n",
       "split2_test_score                                                       0.767635   \n",
       "split3_test_score                                                       0.759336   \n",
       "split4_test_score                                                       0.721992   \n",
       "mean_test_score                                                         0.748966   \n",
       "std_test_score                                                          0.020197   \n",
       "rank_test_score                                                                2   \n",
       "split0_train_score                                                      0.751295   \n",
       "split1_train_score                                                      0.748187   \n",
       "split2_train_score                                                      0.746377   \n",
       "split3_train_score                                                      0.756729   \n",
       "split4_train_score                                                      0.758799   \n",
       "mean_train_score                                                        0.752277   \n",
       "std_train_score                                                         0.004793   \n",
       "\n",
       "                                                                             297  \\\n",
       "mean_fit_time                                                           0.007604   \n",
       "std_fit_time                                                            0.000492   \n",
       "mean_score_time                                                         0.001402   \n",
       "std_score_time                                                           0.00049   \n",
       "param_classifier                                            LogisticRegression()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                             StandardScaler()   \n",
       "param_classifier__penalty                                                     l2   \n",
       "param_classifier__solver                                               newton-cg   \n",
       "param_classifier__alpha                                                      NaN   \n",
       "params                         {'classifier': LogisticRegression(), 'classifi...   \n",
       "split0_test_score                                                       0.727273   \n",
       "split1_test_score                                                       0.768595   \n",
       "split2_test_score                                                       0.771784   \n",
       "split3_test_score                                                       0.759336   \n",
       "split4_test_score                                                       0.717842   \n",
       "mean_test_score                                                         0.748966   \n",
       "std_test_score                                                          0.022149   \n",
       "rank_test_score                                                                2   \n",
       "split0_train_score                                                      0.750259   \n",
       "split1_train_score                                                       0.74715   \n",
       "split2_train_score                                                      0.746377   \n",
       "split3_train_score                                                      0.753623   \n",
       "split4_train_score                                                      0.754658   \n",
       "mean_train_score                                                        0.750414   \n",
       "std_train_score                                                         0.003325   \n",
       "\n",
       "                                                                             310  \\\n",
       "mean_fit_time                                                           0.016805   \n",
       "std_fit_time                                                            0.000742   \n",
       "mean_score_time                                                         0.001801   \n",
       "std_score_time                                                            0.0004   \n",
       "param_classifier                                            LogisticRegression()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                               RobustScaler()   \n",
       "param_classifier__penalty                                                     l2   \n",
       "param_classifier__solver                                                     sag   \n",
       "param_classifier__alpha                                                      NaN   \n",
       "params                         {'classifier': LogisticRegression(), 'classifi...   \n",
       "split0_test_score                                                       0.727273   \n",
       "split1_test_score                                                       0.772727   \n",
       "split2_test_score                                                       0.767635   \n",
       "split3_test_score                                                       0.759336   \n",
       "split4_test_score                                                       0.717842   \n",
       "mean_test_score                                                         0.748963   \n",
       "std_test_score                                                          0.022181   \n",
       "rank_test_score                                                                4   \n",
       "split0_train_score                                                      0.749223   \n",
       "split1_train_score                                                      0.748187   \n",
       "split2_train_score                                                      0.743271   \n",
       "split3_train_score                                                      0.754658   \n",
       "split4_train_score                                                      0.754658   \n",
       "mean_train_score                                                        0.749999   \n",
       "std_train_score                                                         0.004303   \n",
       "\n",
       "                                                                             289  \\\n",
       "mean_fit_time                                                           0.007601   \n",
       "std_fit_time                                                              0.0008   \n",
       "mean_score_time                                                         0.001802   \n",
       "std_score_time                                                          0.000401   \n",
       "param_classifier                                            LogisticRegression()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                             StandardScaler()   \n",
       "param_classifier__penalty                                                     l1   \n",
       "param_classifier__solver                                               liblinear   \n",
       "param_classifier__alpha                                                      NaN   \n",
       "params                         {'classifier': LogisticRegression(), 'classifi...   \n",
       "split0_test_score                                                       0.731405   \n",
       "split1_test_score                                                       0.768595   \n",
       "split2_test_score                                                       0.771784   \n",
       "split3_test_score                                                       0.759336   \n",
       "split4_test_score                                                       0.713693   \n",
       "mean_test_score                                                         0.748963   \n",
       "std_test_score                                                          0.022654   \n",
       "rank_test_score                                                                4   \n",
       "split0_train_score                                                      0.751295   \n",
       "split1_train_score                                                      0.745078   \n",
       "split2_train_score                                                      0.744306   \n",
       "split3_train_score                                                      0.755694   \n",
       "split4_train_score                                                      0.755694   \n",
       "mean_train_score                                                        0.750413   \n",
       "std_train_score                                                         0.004946   \n",
       "\n",
       "                                                                             298  \\\n",
       "mean_fit_time                                                           0.010001   \n",
       "std_fit_time                                                            0.000627   \n",
       "mean_score_time                                                         0.001801   \n",
       "std_score_time                                                          0.000401   \n",
       "param_classifier                                            LogisticRegression()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                               RobustScaler()   \n",
       "param_classifier__penalty                                                     l2   \n",
       "param_classifier__solver                                               newton-cg   \n",
       "param_classifier__alpha                                                      NaN   \n",
       "params                         {'classifier': LogisticRegression(), 'classifi...   \n",
       "split0_test_score                                                       0.727273   \n",
       "split1_test_score                                                       0.768595   \n",
       "split2_test_score                                                       0.767635   \n",
       "split3_test_score                                                       0.759336   \n",
       "split4_test_score                                                       0.717842   \n",
       "mean_test_score                                                         0.748136   \n",
       "std_test_score                                                          0.021341   \n",
       "rank_test_score                                                                6   \n",
       "split0_train_score                                                      0.748187   \n",
       "split1_train_score                                                      0.748187   \n",
       "split2_train_score                                                      0.743271   \n",
       "split3_train_score                                                      0.754658   \n",
       "split4_train_score                                                      0.754658   \n",
       "mean_train_score                                                        0.749792   \n",
       "std_train_score                                                          0.00436   \n",
       "\n",
       "                                                                             301  \\\n",
       "mean_fit_time                                                           0.005205   \n",
       "std_fit_time                                                            0.000397   \n",
       "mean_score_time                                                         0.001598   \n",
       "std_score_time                                                          0.000488   \n",
       "param_classifier                                            LogisticRegression()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                             StandardScaler()   \n",
       "param_classifier__penalty                                                     l2   \n",
       "param_classifier__solver                                                   lbfgs   \n",
       "param_classifier__alpha                                                      NaN   \n",
       "params                         {'classifier': LogisticRegression(), 'classifi...   \n",
       "split0_test_score                                                       0.727273   \n",
       "split1_test_score                                                       0.768595   \n",
       "split2_test_score                                                       0.767635   \n",
       "split3_test_score                                                       0.759336   \n",
       "split4_test_score                                                       0.717842   \n",
       "mean_test_score                                                         0.748136   \n",
       "std_test_score                                                          0.021341   \n",
       "rank_test_score                                                                6   \n",
       "split0_train_score                                                      0.750259   \n",
       "split1_train_score                                                       0.74715   \n",
       "split2_train_score                                                      0.746377   \n",
       "split3_train_score                                                      0.753623   \n",
       "split4_train_score                                                      0.754658   \n",
       "mean_train_score                                                        0.750414   \n",
       "std_train_score                                                         0.003325   \n",
       "\n",
       "                                                                             302  \\\n",
       "mean_fit_time                                                           0.007596   \n",
       "std_fit_time                                                            0.000793   \n",
       "mean_score_time                                                         0.001203   \n",
       "std_score_time                                                          0.000399   \n",
       "param_classifier                                            LogisticRegression()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                               RobustScaler()   \n",
       "param_classifier__penalty                                                     l2   \n",
       "param_classifier__solver                                                   lbfgs   \n",
       "param_classifier__alpha                                                      NaN   \n",
       "params                         {'classifier': LogisticRegression(), 'classifi...   \n",
       "split0_test_score                                                       0.727273   \n",
       "split1_test_score                                                       0.768595   \n",
       "split2_test_score                                                       0.767635   \n",
       "split3_test_score                                                       0.759336   \n",
       "split4_test_score                                                       0.717842   \n",
       "mean_test_score                                                         0.748136   \n",
       "std_test_score                                                          0.021341   \n",
       "rank_test_score                                                                6   \n",
       "split0_train_score                                                      0.748187   \n",
       "split1_train_score                                                      0.748187   \n",
       "split2_train_score                                                      0.743271   \n",
       "split3_train_score                                                      0.754658   \n",
       "split4_train_score                                                      0.754658   \n",
       "mean_train_score                                                        0.749792   \n",
       "std_train_score                                                          0.00436   \n",
       "\n",
       "                                                                             305  \\\n",
       "mean_fit_time                                                           0.004201   \n",
       "std_fit_time                                                            0.000399   \n",
       "mean_score_time                                                           0.0012   \n",
       "std_score_time                                                          0.000401   \n",
       "param_classifier                                            LogisticRegression()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                             StandardScaler()   \n",
       "param_classifier__penalty                                                     l2   \n",
       "param_classifier__solver                                               liblinear   \n",
       "param_classifier__alpha                                                      NaN   \n",
       "params                         {'classifier': LogisticRegression(), 'classifi...   \n",
       "split0_test_score                                                       0.727273   \n",
       "split1_test_score                                                       0.768595   \n",
       "split2_test_score                                                       0.767635   \n",
       "split3_test_score                                                       0.759336   \n",
       "split4_test_score                                                       0.717842   \n",
       "mean_test_score                                                         0.748136   \n",
       "std_test_score                                                          0.021341   \n",
       "rank_test_score                                                                6   \n",
       "split0_train_score                                                      0.749223   \n",
       "split1_train_score                                                       0.74715   \n",
       "split2_train_score                                                      0.745342   \n",
       "split3_train_score                                                      0.755694   \n",
       "split4_train_score                                                      0.757764   \n",
       "mean_train_score                                                        0.751034   \n",
       "std_train_score                                                         0.004853   \n",
       "\n",
       "                                                                             306  \\\n",
       "mean_fit_time                                                           0.005202   \n",
       "std_fit_time                                                            0.000402   \n",
       "mean_score_time                                                         0.001198   \n",
       "std_score_time                                                          0.000396   \n",
       "param_classifier                                            LogisticRegression()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                               RobustScaler()   \n",
       "param_classifier__penalty                                                     l2   \n",
       "param_classifier__solver                                               liblinear   \n",
       "param_classifier__alpha                                                      NaN   \n",
       "params                         {'classifier': LogisticRegression(), 'classifi...   \n",
       "split0_test_score                                                       0.727273   \n",
       "split1_test_score                                                       0.768595   \n",
       "split2_test_score                                                       0.767635   \n",
       "split3_test_score                                                       0.759336   \n",
       "split4_test_score                                                       0.717842   \n",
       "mean_test_score                                                         0.748136   \n",
       "std_test_score                                                          0.021341   \n",
       "rank_test_score                                                                6   \n",
       "split0_train_score                                                      0.749223   \n",
       "split1_train_score                                                      0.750259   \n",
       "split2_train_score                                                      0.743271   \n",
       "split3_train_score                                                      0.754658   \n",
       "split4_train_score                                                      0.756729   \n",
       "mean_train_score                                                        0.750828   \n",
       "std_train_score                                                          0.00468   \n",
       "\n",
       "                               ...  \\\n",
       "mean_fit_time                  ...   \n",
       "std_fit_time                   ...   \n",
       "mean_score_time                ...   \n",
       "std_score_time                 ...   \n",
       "param_classifier               ...   \n",
       "param_classifier__n_neighbors  ...   \n",
       "param_classifier__p            ...   \n",
       "param_classifier__weights      ...   \n",
       "param_preprocessing            ...   \n",
       "param_classifier__penalty      ...   \n",
       "param_classifier__solver       ...   \n",
       "param_classifier__alpha        ...   \n",
       "params                         ...   \n",
       "split0_test_score              ...   \n",
       "split1_test_score              ...   \n",
       "split2_test_score              ...   \n",
       "split3_test_score              ...   \n",
       "split4_test_score              ...   \n",
       "mean_test_score                ...   \n",
       "std_test_score                 ...   \n",
       "rank_test_score                ...   \n",
       "split0_train_score             ...   \n",
       "split1_train_score             ...   \n",
       "split2_train_score             ...   \n",
       "split3_train_score             ...   \n",
       "split4_train_score             ...   \n",
       "mean_train_score               ...   \n",
       "std_train_score                ...   \n",
       "\n",
       "                                                                             357  \\\n",
       "mean_fit_time                                                           0.003001   \n",
       "std_fit_time                                                            0.000633   \n",
       "mean_score_time                                                              0.0   \n",
       "std_score_time                                                               0.0   \n",
       "param_classifier                                                 MultinomialNB()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                             StandardScaler()   \n",
       "param_classifier__penalty                                                    NaN   \n",
       "param_classifier__solver                                                     NaN   \n",
       "param_classifier__alpha                                                      0.6   \n",
       "params                         {'classifier': MultinomialNB(), 'classifier__a...   \n",
       "split0_test_score                                                            NaN   \n",
       "split1_test_score                                                            NaN   \n",
       "split2_test_score                                                            NaN   \n",
       "split3_test_score                                                            NaN   \n",
       "split4_test_score                                                            NaN   \n",
       "mean_test_score                                                              NaN   \n",
       "std_test_score                                                               NaN   \n",
       "rank_test_score                                                              397   \n",
       "split0_train_score                                                           NaN   \n",
       "split1_train_score                                                           NaN   \n",
       "split2_train_score                                                           NaN   \n",
       "split3_train_score                                                           NaN   \n",
       "split4_train_score                                                           NaN   \n",
       "mean_train_score                                                             NaN   \n",
       "std_train_score                                                              NaN   \n",
       "\n",
       "                                                                             358  \\\n",
       "mean_fit_time                                                           0.004401   \n",
       "std_fit_time                                                             0.00049   \n",
       "mean_score_time                                                              0.0   \n",
       "std_score_time                                                               0.0   \n",
       "param_classifier                                                 MultinomialNB()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                               RobustScaler()   \n",
       "param_classifier__penalty                                                    NaN   \n",
       "param_classifier__solver                                                     NaN   \n",
       "param_classifier__alpha                                                      0.6   \n",
       "params                         {'classifier': MultinomialNB(), 'classifier__a...   \n",
       "split0_test_score                                                            NaN   \n",
       "split1_test_score                                                            NaN   \n",
       "split2_test_score                                                            NaN   \n",
       "split3_test_score                                                            NaN   \n",
       "split4_test_score                                                            NaN   \n",
       "mean_test_score                                                              NaN   \n",
       "std_test_score                                                               NaN   \n",
       "rank_test_score                                                              397   \n",
       "split0_train_score                                                           NaN   \n",
       "split1_train_score                                                           NaN   \n",
       "split2_train_score                                                           NaN   \n",
       "split3_train_score                                                           NaN   \n",
       "split4_train_score                                                           NaN   \n",
       "mean_train_score                                                             NaN   \n",
       "std_train_score                                                              NaN   \n",
       "\n",
       "                                                                             361  \\\n",
       "mean_fit_time                                                           0.003201   \n",
       "std_fit_time                                                            0.000401   \n",
       "mean_score_time                                                              0.0   \n",
       "std_score_time                                                               0.0   \n",
       "param_classifier                                                 MultinomialNB()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                             StandardScaler()   \n",
       "param_classifier__penalty                                                    NaN   \n",
       "param_classifier__solver                                                     NaN   \n",
       "param_classifier__alpha                                                      0.7   \n",
       "params                         {'classifier': MultinomialNB(), 'classifier__a...   \n",
       "split0_test_score                                                            NaN   \n",
       "split1_test_score                                                            NaN   \n",
       "split2_test_score                                                            NaN   \n",
       "split3_test_score                                                            NaN   \n",
       "split4_test_score                                                            NaN   \n",
       "mean_test_score                                                              NaN   \n",
       "std_test_score                                                               NaN   \n",
       "rank_test_score                                                              397   \n",
       "split0_train_score                                                           NaN   \n",
       "split1_train_score                                                           NaN   \n",
       "split2_train_score                                                           NaN   \n",
       "split3_train_score                                                           NaN   \n",
       "split4_train_score                                                           NaN   \n",
       "mean_train_score                                                             NaN   \n",
       "std_train_score                                                              NaN   \n",
       "\n",
       "                                                                             362  \\\n",
       "mean_fit_time                                                           0.004201   \n",
       "std_fit_time                                                              0.0004   \n",
       "mean_score_time                                                              0.0   \n",
       "std_score_time                                                               0.0   \n",
       "param_classifier                                                 MultinomialNB()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                               RobustScaler()   \n",
       "param_classifier__penalty                                                    NaN   \n",
       "param_classifier__solver                                                     NaN   \n",
       "param_classifier__alpha                                                      0.7   \n",
       "params                         {'classifier': MultinomialNB(), 'classifier__a...   \n",
       "split0_test_score                                                            NaN   \n",
       "split1_test_score                                                            NaN   \n",
       "split2_test_score                                                            NaN   \n",
       "split3_test_score                                                            NaN   \n",
       "split4_test_score                                                            NaN   \n",
       "mean_test_score                                                              NaN   \n",
       "std_test_score                                                               NaN   \n",
       "rank_test_score                                                              397   \n",
       "split0_train_score                                                           NaN   \n",
       "split1_train_score                                                           NaN   \n",
       "split2_train_score                                                           NaN   \n",
       "split3_train_score                                                           NaN   \n",
       "split4_train_score                                                           NaN   \n",
       "mean_train_score                                                             NaN   \n",
       "std_train_score                                                              NaN   \n",
       "\n",
       "                                                                             365  \\\n",
       "mean_fit_time                                                           0.003001   \n",
       "std_fit_time                                                                 0.0   \n",
       "mean_score_time                                                              0.0   \n",
       "std_score_time                                                               0.0   \n",
       "param_classifier                                                 MultinomialNB()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                             StandardScaler()   \n",
       "param_classifier__penalty                                                    NaN   \n",
       "param_classifier__solver                                                     NaN   \n",
       "param_classifier__alpha                                                      0.8   \n",
       "params                         {'classifier': MultinomialNB(), 'classifier__a...   \n",
       "split0_test_score                                                            NaN   \n",
       "split1_test_score                                                            NaN   \n",
       "split2_test_score                                                            NaN   \n",
       "split3_test_score                                                            NaN   \n",
       "split4_test_score                                                            NaN   \n",
       "mean_test_score                                                              NaN   \n",
       "std_test_score                                                               NaN   \n",
       "rank_test_score                                                              397   \n",
       "split0_train_score                                                           NaN   \n",
       "split1_train_score                                                           NaN   \n",
       "split2_train_score                                                           NaN   \n",
       "split3_train_score                                                           NaN   \n",
       "split4_train_score                                                           NaN   \n",
       "mean_train_score                                                             NaN   \n",
       "std_train_score                                                              NaN   \n",
       "\n",
       "                                                                             366  \\\n",
       "mean_fit_time                                                           0.004401   \n",
       "std_fit_time                                                             0.00049   \n",
       "mean_score_time                                                              0.0   \n",
       "std_score_time                                                               0.0   \n",
       "param_classifier                                                 MultinomialNB()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                               RobustScaler()   \n",
       "param_classifier__penalty                                                    NaN   \n",
       "param_classifier__solver                                                     NaN   \n",
       "param_classifier__alpha                                                      0.8   \n",
       "params                         {'classifier': MultinomialNB(), 'classifier__a...   \n",
       "split0_test_score                                                            NaN   \n",
       "split1_test_score                                                            NaN   \n",
       "split2_test_score                                                            NaN   \n",
       "split3_test_score                                                            NaN   \n",
       "split4_test_score                                                            NaN   \n",
       "mean_test_score                                                              NaN   \n",
       "std_test_score                                                               NaN   \n",
       "rank_test_score                                                              397   \n",
       "split0_train_score                                                           NaN   \n",
       "split1_train_score                                                           NaN   \n",
       "split2_train_score                                                           NaN   \n",
       "split3_train_score                                                           NaN   \n",
       "split4_train_score                                                           NaN   \n",
       "mean_train_score                                                             NaN   \n",
       "std_train_score                                                              NaN   \n",
       "\n",
       "                                                                             369  \\\n",
       "mean_fit_time                                                           0.003201   \n",
       "std_fit_time                                                            0.000401   \n",
       "mean_score_time                                                              0.0   \n",
       "std_score_time                                                               0.0   \n",
       "param_classifier                                                 MultinomialNB()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                             StandardScaler()   \n",
       "param_classifier__penalty                                                    NaN   \n",
       "param_classifier__solver                                                     NaN   \n",
       "param_classifier__alpha                                                      0.9   \n",
       "params                         {'classifier': MultinomialNB(), 'classifier__a...   \n",
       "split0_test_score                                                            NaN   \n",
       "split1_test_score                                                            NaN   \n",
       "split2_test_score                                                            NaN   \n",
       "split3_test_score                                                            NaN   \n",
       "split4_test_score                                                            NaN   \n",
       "mean_test_score                                                              NaN   \n",
       "std_test_score                                                               NaN   \n",
       "rank_test_score                                                              397   \n",
       "split0_train_score                                                           NaN   \n",
       "split1_train_score                                                           NaN   \n",
       "split2_train_score                                                           NaN   \n",
       "split3_train_score                                                           NaN   \n",
       "split4_train_score                                                           NaN   \n",
       "mean_train_score                                                             NaN   \n",
       "std_train_score                                                              NaN   \n",
       "\n",
       "                                                                             370  \\\n",
       "mean_fit_time                                                           0.004401   \n",
       "std_fit_time                                                             0.00049   \n",
       "mean_score_time                                                              0.0   \n",
       "std_score_time                                                               0.0   \n",
       "param_classifier                                                 MultinomialNB()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                               RobustScaler()   \n",
       "param_classifier__penalty                                                    NaN   \n",
       "param_classifier__solver                                                     NaN   \n",
       "param_classifier__alpha                                                      0.9   \n",
       "params                         {'classifier': MultinomialNB(), 'classifier__a...   \n",
       "split0_test_score                                                            NaN   \n",
       "split1_test_score                                                            NaN   \n",
       "split2_test_score                                                            NaN   \n",
       "split3_test_score                                                            NaN   \n",
       "split4_test_score                                                            NaN   \n",
       "mean_test_score                                                              NaN   \n",
       "std_test_score                                                               NaN   \n",
       "rank_test_score                                                              397   \n",
       "split0_train_score                                                           NaN   \n",
       "split1_train_score                                                           NaN   \n",
       "split2_train_score                                                           NaN   \n",
       "split3_train_score                                                           NaN   \n",
       "split4_train_score                                                           NaN   \n",
       "mean_train_score                                                             NaN   \n",
       "std_train_score                                                              NaN   \n",
       "\n",
       "                                                                             373  \\\n",
       "mean_fit_time                                                           0.002801   \n",
       "std_fit_time                                                              0.0004   \n",
       "mean_score_time                                                              0.0   \n",
       "std_score_time                                                               0.0   \n",
       "param_classifier                                                 MultinomialNB()   \n",
       "param_classifier__n_neighbors                                                NaN   \n",
       "param_classifier__p                                                          NaN   \n",
       "param_classifier__weights                                                    NaN   \n",
       "param_preprocessing                                             StandardScaler()   \n",
       "param_classifier__penalty                                                    NaN   \n",
       "param_classifier__solver                                                     NaN   \n",
       "param_classifier__alpha                                                      1.0   \n",
       "params                         {'classifier': MultinomialNB(), 'classifier__a...   \n",
       "split0_test_score                                                            NaN   \n",
       "split1_test_score                                                            NaN   \n",
       "split2_test_score                                                            NaN   \n",
       "split3_test_score                                                            NaN   \n",
       "split4_test_score                                                            NaN   \n",
       "mean_test_score                                                              NaN   \n",
       "std_test_score                                                               NaN   \n",
       "rank_test_score                                                              397   \n",
       "split0_train_score                                                           NaN   \n",
       "split1_train_score                                                           NaN   \n",
       "split2_train_score                                                           NaN   \n",
       "split3_train_score                                                           NaN   \n",
       "split4_train_score                                                           NaN   \n",
       "mean_train_score                                                             NaN   \n",
       "std_train_score                                                              NaN   \n",
       "\n",
       "                                                                             374  \n",
       "mean_fit_time                                                           0.004601  \n",
       "std_fit_time                                                             0.00049  \n",
       "mean_score_time                                                              0.0  \n",
       "std_score_time                                                               0.0  \n",
       "param_classifier                                                 MultinomialNB()  \n",
       "param_classifier__n_neighbors                                                NaN  \n",
       "param_classifier__p                                                          NaN  \n",
       "param_classifier__weights                                                    NaN  \n",
       "param_preprocessing                                               RobustScaler()  \n",
       "param_classifier__penalty                                                    NaN  \n",
       "param_classifier__solver                                                     NaN  \n",
       "param_classifier__alpha                                                      1.0  \n",
       "params                         {'classifier': MultinomialNB(), 'classifier__a...  \n",
       "split0_test_score                                                            NaN  \n",
       "split1_test_score                                                            NaN  \n",
       "split2_test_score                                                            NaN  \n",
       "split3_test_score                                                            NaN  \n",
       "split4_test_score                                                            NaN  \n",
       "mean_test_score                                                              NaN  \n",
       "std_test_score                                                               NaN  \n",
       "rank_test_score                                                              397  \n",
       "split0_train_score                                                           NaN  \n",
       "split1_train_score                                                           NaN  \n",
       "split2_train_score                                                           NaN  \n",
       "split3_train_score                                                           NaN  \n",
       "split4_train_score                                                           NaN  \n",
       "mean_train_score                                                             NaN  \n",
       "std_train_score                                                              NaN  \n",
       "\n",
       "[28 rows x 416 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result = pd.DataFrame(grid.cv_results_).sort_values([\"rank_test_score\",'std_test_score']).T\n",
    "grid_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Игра \"Найди 10 отличий в боксплотах\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\for conda\\lib\\site-packages\\seaborn\\categorical.py:82: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  plot_data = [np.asarray(s, float) for k, s in iter_data]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = grid_result.index[grid_result.index.str.endswith(\"_test_score\")][:-3]\n",
    "tabl = grid_result.loc[mask].iloc[:,:10]\n",
    "sns.boxplot(data=tabl);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Посмотрел на пороговое значение, удалось улучшить модель на пару процентов, главное не заигрываться и не выкручивать его на максимум, да, пресижн будет расти еще сильнее (немного) но реколу становится очень плохо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = grid.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "threshold = 0.56\n",
    "y_pred = (y_proba >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Посмотрю на матрицу ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(matrix, annot=True, fmt='g', cbar=None, cmap=\"Blues\", \n",
    "            xticklabels=['y_pred =  0', 'y_pred =  1'], yticklabels=['y_test =  0', 'y_test =  1']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывожу 3 основные метрики, смотрю прежде всего на precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall:     0.5128205128205128\n",
      "precision:  0.8\n",
      "f1:         0.625\n"
     ]
    }
   ],
   "source": [
    "print('recall:    ', recall_score(y_test, y_pred))\n",
    "print('precision: ', precision_score(y_test, y_pred))\n",
    "print('f1:        ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Немного красивого графика рок-аук, просто чтобы был"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Из всех моих прогонов выдавали лучшие результаты регрессия и соседи, но 0.8 пресижн при 0.51 реколе не было еще ( в среднем 0.7 пресижн 0.6 рекол)\n",
    "## Использую модель: ({'classifier': LogisticRegression(), 'classifier__penalty': 'none', 'classifier__solver': 'sag', 'preprocessing': RobustScaler()},)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
